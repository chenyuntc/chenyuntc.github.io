<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications | Yun Chen</title>
    <link>https://tmux.top/publication/</link>
      <atom:link href="https://tmux.top/publication/index.xml" rel="self" type="application/rss+xml" />
    <description>Publications</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://tmux.top/media/icon_hu_e01690ce589688e5.png</url>
      <title>Publications</title>
      <link>https://tmux.top/publication/</link>
    </image>
    
    <item>
      <title>G3R: Gradient Guided Generalizable Reconstruction</title>
      <link>https://tmux.top/publication/g3r/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/g3r/</guid>
      <description>&lt;meta http-equiv=&#34;refresh&#34; content=&#34;0;URL=&#39;https://waabi.ai/g3r/&#39;&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>Real-Time Neural Rasterization for Large Scenes</title>
      <link>https://tmux.top/publication/neuras/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/neuras/</guid>
      <description>&lt;meta http-equiv=&#34;refresh&#34; content=&#34;0;URL=&#39;https://waabi.ai/neuras/&#39;&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>UniSim: A Neural Closed-Loop Sensor Simulator</title>
      <link>https://tmux.top/publication/unisim/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/unisim/</guid>
      <description>&lt;meta http-equiv=&#34;refresh&#34; content=&#34;0;URL=&#39;https://waabi.ai/unisim/&#39;&#34; /&gt;
</description>
    </item>
    
    <item>
      <title>GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving</title>
      <link>https://tmux.top/publication/geosim/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/geosim/</guid>
      <description>&lt;p style=&#34;color:red;&#34;&gt; &lt;b&gt; GeoSim is nominated as a best paper candidate in CVPR &lt;/b&gt;&lt;/p&gt;
&lt;h3 id=&#34;overview-video&#34;&gt;Overview Video&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/_VLXc_VN0fE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube&lt;/a&gt; or &lt;a href=&#34;https://www.bilibili.com/video/BV1154y137jC/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bilibili&lt;/a&gt;. You may also download it using this &lt;a href=&#34;http://yun.sfo2.digitaloceanspaces.com/public/geosim/video-5min.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/_VLXc_VN0fE?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present GeoSim, a geometry-guided simulation procedure to insert dynamic objects into videos with greater realism. It helps enable scalable sensor simulation for training and testing autonomy systems, as well as applications like AR, VR, and video editing.
We first create 3D assets from real-world data and then use the proposed simulation pipeline to realistically composite 3D assets into existing camera imagery.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;alternative text for search engines&#34;
           src=&#34;https://tmux.top/publication/geosim/featured.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;3d-asset-reconstruction&#34;&gt;3D asset reconstruction&lt;/h3&gt;
&lt;p&gt;We propose a self-supervision model to automatically build vehicle asset bank from the wild without 3D groundtruth.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Multi-sensor feature is extracted from camera and LiDAR.&lt;/li&gt;
&lt;li&gt;Per-vertex deformation is applied on a learnable mean-shape to predict shape.&lt;/li&gt;
&lt;li&gt;Silhouette is differentiably rendered from the predicted mesh and compared with the labels. LiDAR consisteny is also applied&lt;/li&gt;
&lt;/ol&gt;


















&lt;figure  id=&#34;figure-self-supervised-mesh-reconstruction&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Self-supervised mesh reconstruction&#34; srcset=&#34;
               /publication/geosim/imgs/meshnet_hu_12a0c245d7992b65.webp 400w,
               /publication/geosim/imgs/meshnet_hu_a30bdf1dd7d23dc3.webp 760w,
               /publication/geosim/imgs/meshnet_hu_42431582baa04323.webp 1200w&#34;
               src=&#34;https://tmux.top/publication/geosim/imgs/meshnet_hu_12a0c245d7992b65.webp&#34;
               width=&#34;760&#34;
               height=&#34;259&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Self-supervised mesh reconstruction
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Using the reconstructed 3D mesh, we can perform novel view warping of the source image to new target poses.&lt;/p&gt;
&lt;p&gt;We build a large and diverse asset bank for simulation with over 8 000 unique vehicles.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Asset 1&#34;
           src=&#34;https://tmux.top/publication/geosim/imgs/1.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Asset 2&#34;
           src=&#34;https://tmux.top/publication/geosim/imgs/2.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Asset 3&#34;
           src=&#34;https://tmux.top/publication/geosim/imgs/0.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;!-- ![Asset 4](imgs/4.gif) --&gt;
&lt;h3 id=&#34;simulation-pipeline&#34;&gt;Simulation Pipeline&lt;/h3&gt;
&lt;p&gt;GeoSim leverages the asset library to insert actors and simulate new images.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Step 1: &lt;strong&gt;Scenario Generation&lt;/strong&gt;
We first automatically generate a plausible placement and trajectory that complies with the existing traffic.  An asset with a similar viewpoint is selected for rendering.


















&lt;figure  id=&#34;figure-automatic-scenario-generation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Automatic scenario generation&#34; srcset=&#34;
               /publication/geosim/imgs/scengen_hu_9352e6c02cee657.webp 400w,
               /publication/geosim/imgs/scengen_hu_39915ab26c6a8ae1.webp 760w,
               /publication/geosim/imgs/scengen_hu_9f9bb2b663bd37db.webp 1200w&#34;
               src=&#34;https://tmux.top/publication/geosim/imgs/scengen_hu_9352e6c02cee657.webp&#34;
               width=&#34;760&#34;
               height=&#34;302&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Automatic scenario generation
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 2: &lt;strong&gt;Occlussion-aware rendering&lt;/strong&gt;
We render the selected asset at a new target pose.
We account for occlusion by using dense depth from a pre-trained depth completion network.
Additionally,  we render a soft-shadow corresponding to the selected 3D asset considering a cloudy HDRI as the environement map..


















&lt;figure  id=&#34;figure-occlussion-aware-rendering&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Occlussion-aware rendering&#34; srcset=&#34;
               /publication/geosim/imgs/occ-render_hu_9897d6d2386e804f.webp 400w,
               /publication/geosim/imgs/occ-render_hu_837673f943baedca.webp 760w,
               /publication/geosim/imgs/occ-render_hu_b4b6dcc25b81ec65.webp 1200w&#34;
               src=&#34;https://tmux.top/publication/geosim/imgs/occ-render_hu_9897d6d2386e804f.webp&#34;
               width=&#34;760&#34;
               height=&#34;216&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Occlussion-aware rendering
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 3: &lt;strong&gt;Post composition:&lt;/strong&gt; Finally, we perform post-image composition. We use a synthesis network to handle inconsistent illumination and inpaint the discrepancies at the boundary,  so that the vehicle is composited into the image seamlessly.


















&lt;figure  id=&#34;figure-post-image-composition&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Post-image composition&#34; srcset=&#34;
               /publication/geosim/imgs/postcom_hu_8d5408a4a189c3c1.webp 400w,
               /publication/geosim/imgs/postcom_hu_58e825ba9ff6becb.webp 760w,
               /publication/geosim/imgs/postcom_hu_f04b925938f8160d.webp 1200w&#34;
               src=&#34;https://tmux.top/publication/geosim/imgs/postcom_hu_8d5408a4a189c3c1.webp&#34;
               width=&#34;760&#34;
               height=&#34;235&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Post-image composition
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;qualitative-results&#34;&gt;Qualitative Results&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://yun.sfo2.digitaloceanspaces.com/public/geosim/geosim-4K.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video results in 4K resolution (4096x2160)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://yun.sfo2.digitaloceanspaces.com/public/geosim/geosim-4K.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;4K video&#34; srcset=&#34;
               /publication/geosim/imgs/4KVideo_hu_658be2148e60782b.webp 400w,
               /publication/geosim/imgs/4KVideo_hu_ec7679c7fcc4e136.webp 760w,
               /publication/geosim/imgs/4KVideo_hu_977f290df4483f30.webp 1200w&#34;
               src=&#34;https://tmux.top/publication/geosim/imgs/4KVideo_hu_658be2148e60782b.webp&#34;
               width=&#34;760&#34;
               height=&#34;343&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ExampleSmall&#34;
           src=&#34;https://tmux.top/publication/geosim/imgs/c4-small.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;@inproceedings{chen2021geosim,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; title={GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; author={Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, Raquel Urtasun},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; year={2021},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; booktitle={CVPR},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Lane Graph Representations for Motion Forecasting</title>
      <link>https://tmux.top/publication/lanegcn/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/lanegcn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dsdnet: Deep structured self-driving network</title>
      <link>https://tmux.top/publication/dsdnet/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/dsdnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PnPNet: End-to-End Perception and Prediction with Tracking in the Loop</title>
      <link>https://tmux.top/publication/pnpnet/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/pnpnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Joint 2D-3D Representations for Depth Completion</title>
      <link>https://tmux.top/publication/fusenet/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/fusenet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-task multi-sensor fusion for 3d object detection</title>
      <link>https://tmux.top/publication/mmf/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/mmf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Volume R-CNN: Unified Framework for CT Object Detection and Instance Segmentation</title>
      <link>https://tmux.top/publication/vrcnn/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/vrcnn/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
