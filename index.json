[{"authors":null,"categories":null,"content":"Yun Chen is a researcher at Uber ATG R\u0026amp;D led by Raquel Urtasun and worked closely with Shenlong Wang, Ming Liang and Bin Yang.\nHe is a follower of Unix philosophy, an advocator of Linux, a geek of Android, the author of a PyTorch best-seller, and open-source contributor.\nHe has set new state-of-the-art for several tasks (including Autonomy/NLP/Vision), also served as reviewer for CVPR, ACCV, WACV and RA-L.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://tmux.top/author/yun-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yun-chen/","section":"authors","summary":"Yun Chen is a researcher at Uber ATG R\u0026amp;D led by Raquel Urtasun and worked closely with Shenlong Wang, Ming Liang and Bin Yang.\nHe is a follower of Unix philosophy, an advocator of Linux, a geek of Android, the author of a PyTorch best-seller, and open-source contributor.","tags":null,"title":"Yun Chen","type":"authors"},{"authors":["Yun Chen#","Frieda Rong#","Shivam Duggal#","Shenlong Wang","Xinchen Yan","Sivabalan Manivasagam","Shangjie Xue","Ersin Yumer","Raquel Urtasun"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"5f7982cda567ba3a966c271915f69c90","permalink":"https://tmux.top/publication/geosim/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/geosim/","section":"publication","summary":"A geometry-aware image composition process (GeoSim) that synthesizes novel urban driving scenes by augmenting existing images with dynamic objects extracted from other scenes and rendered at novel poses.","tags":null,"title":"GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving","type":"publication"},{"authors":["Ming Liang","Bin Yang","Rui Hu","Yun Chen","Renjie Liao","Song Feng","Raquel Urtasun"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1599004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599004800,"objectID":"22141d42bda5b3d90d8dde616c67df7b","permalink":"https://tmux.top/publication/lanegcn/","publishdate":"2020-09-02T00:00:00Z","relpermalink":"/publication/lanegcn/","section":"publication","summary":"\\[ECCV 2020 Oral\\] We propose a motion forecasting model that exploits a novel structured map representation as well as actor-map interactions. Instead of encoding vectorized maps as raster images, we construct a lane graph from raw map data to explicitly preserve the map structure. To capture the complex topology and long range dependencies of the lane graph, we propose LaneGCN which extends graph convolutions with multiple adjacency matrices and along-lane dilation. To capture the complex interactions between actors and maps, we exploit a fusion network consisting of four types of interactions, actor-to-lane, lane-to-lane, laneto-actor and actor-to-actor. Powered by LaneGCN and actor-map interactions, our model is able to predict accurate and realistic multi-modal trajectories. Our approach significantly outperforms the state-of-the-art on the large scale Argoverse motion forecasting benchmark.","tags":null,"title":"Learning Lane Graph Representations for Motion Forecasting","type":"publication"},{"authors":["Wenyuan Zeng","Shenlong Wang","Renjie Liao","Yun Chen","Bin Yang","Raquel Urtasun"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"1644d29a4b992acc136bfff6b8d9a4ac","permalink":"https://tmux.top/publication/dsdnet/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/dsdnet/","section":"publication","summary":"\\[ECCV 2020\\] In this paper, we propose the Deep Structured self-Driving Network (DSDNet), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, DSDNet explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.","tags":null,"title":"Dsdnet: Deep structured self-driving network","type":"publication"},{"authors":["Ming Liang","Bin Yang","Wenyuan Zeng","Yun Chen","Rui Hu","Sergio Casas","Raquel Urtasun"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"50554e279d997e7ce750094800330863","permalink":"https://tmux.top/publication/pnpnet/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/pnpnet/","section":"publication","summary":"\\[CVPR 2020\\]We tackle the problem of joint perception and motion forecasting in the context of self-driving vehicles. Towards this goal we propose PnPNet, an end-to-end model that takes as input sequential sensor data, and outputs at each time step object tracks and their future trajectories. The key component is a novel tracking module that generates object tracks online from detections and exploits trajectory level features for motion forecasting. Specifically, the object tracks get updated at each time step by solving both the data association problem and the trajectory estimation problem. Importantly, the whole model is end-to-end trainable and benefits from joint optimization of all tasks. We validate PnPNet on two large-scale driving datasets, and show significant improvements over the state-of-the-art with better occlusion recovery and more accurate future prediction.","tags":null,"title":"PnPNet: End-to-End Perception and Prediction with Tracking in the Loop","type":"publication"},{"authors":["Yun Chen","Bin Yang","Ming Liang","Raquel Urtasun"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"f627d8488c893f47b3df4f3855fa0bec","permalink":"https://tmux.top/publication/fusenet/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/fusenet/","section":"publication","summary":"\\[ICCV 2019\\] Depth completion using 2D-3D fusion. New SOTA achieved in KITTI.","tags":null,"title":"Learning Joint 2D-3D Representations for Depth Completion","type":"publication"},{"authors":["Ming Liang","Bin Yang","Yun Chen","Rui Hu","Raquel Urtasun"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"db14e0a1b6ad782683eab60b8e139006","permalink":"https://tmux.top/publication/mmf/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/mmf/","section":"publication","summary":"\\[CVPR 2019\\] In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this  goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and birdâ€™s eye view object detection, while being real-time","tags":null,"title":"Multi-task multi-sensor fusion for 3d object detection","type":"publication"},{"authors":["Yun Chen","Junxuan Chen","Bo Xiao","Zhengfang Wu","Ying Chi","Xuansong Xie","Xiansheng Hua"],"categories":null,"content":"  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.      Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.    Supplementary notes can be added here, including code and math. ","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"3eea6329f6d2bd056f06dceffb1d4eb1","permalink":"https://tmux.top/publication/vrcnn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/vrcnn/","section":"publication","summary":"\\[ISBI 2019\\] Accurate and Efficient 3D Nodule detection with 3D R-CNN.","tags":["Source Themes"],"title":"Volume R-CNN: Unified Framework for CT Object Detection and Instance Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"80ad749dd8073174f20a3cb1be9d3c0b","permalink":"https://tmux.top/project/rcnn/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/rcnn/","section":"project","summary":"The **FIRST** pure PyTorch Faster R-CNN implementation, aiming at Simplicity and Readbility. Faster and Better.","tags":null,"title":"Faster R-CNN","type":"project"},{"authors":null,"categories":null,"content":"","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"c0a73ecfceeed4d63e71715ccfd7471e","permalink":"https://tmux.top/project/pytorch-book/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/pytorch-book/","section":"project","summary":"PyTorch book with anime generation, neural style, image caption and so on.","tags":null,"title":"Technical Book","type":"project"},{"authors":null,"categories":null,"content":"Here is a summary of my projects on 3D vision.\n  Camera Simulation for self-driving: Combine 3D reconstruction, image-based rendering and depth completion to create photo-realistic and geometrically consistent videos (Under Review)\n[PDF] [Video (4K Resolution)]\n    Depth Completion by joint-learning of appearance feature and geometry feature. (ICCV 2019)\n[PDF] [Qualitative Video] [Poster\n   Texture Reconstruction in the wild [WIP]\nSelf-supervised texture reconstruction with symmetry, inpainting and differential-rendering.\n  Please contact me if you want to talk in more detail.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"236a40aaf8e28b258151162d1406821e","permalink":"https://tmux.top/post/summary/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/summary/","section":"post","summary":"Here is a summary of my projects on 3D vision.\n  Camera Simulation for self-driving: Combine 3D reconstruction, image-based rendering and depth completion to create photo-realistic and geometrically consistent videos (Under Review)","tags":null,"title":"Projects on 3D Vision.","type":"post"}]