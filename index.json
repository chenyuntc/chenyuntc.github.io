[{"authors":null,"categories":null,"content":"Yun Chen is a researcher at Uber ATG R\u0026amp;D led by Raquel Urtasun and worked closely with Shenlong Wang, Ming Liang and Bin Yang.\nHe is a follower of Unix philosophy, an advocator of Linux, a geek of Android, the author of a PyTorch best-seller, and open-source contributor.\nHe has set new state-of-the-art for several tasks (including Autonomy/NLP/Vision), also served as reviewer for CVPR, ICCV, ACCV, WACV and RA-L.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://tmux.top/author/yun-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yun-chen/","section":"authors","summary":"Yun Chen is a researcher at Uber ATG R\u0026amp;D led by Raquel Urtasun and worked closely with Shenlong Wang, Ming Liang and Bin Yang.\nHe is a follower of Unix philosophy, an advocator of Linux, a geek of Android, the author of a PyTorch best-seller, and open-source contributor.","tags":null,"title":"Yun Chen","type":"authors"},{"authors":["Yun Chen#","Frieda Rong#","Shivam Duggal#","Shenlong Wang","Xinchen Yan","Sivabalan Manivasagam","Shangjie Xue","Ersin Yumer","Raquel Urtasun"],"categories":null,"content":"Overview Video Youtube or Bilibili. You may also download it using this link   Abstract We present GeoSim, a geometry-guided simulation procedure to insert dynamic objects into videos with greater realism. It helps enable scalable sensor simulation for training and testing autonomy systems, as well as applications like AR, VR, and video editing. We first create 3D assets from real-world data and then use the proposed simulation pipeline to realistically composite 3D assets into existing camera imagery.\n3D asset reconstruction We propose a self-supervision model to automatically build vehicle asset bank from the wild without 3D groundtruth.\n Multi-sensor feature is extracted from camera and LiDAR. Per-vertex deformation is applied on a learnable mean-shape to predict shape. Silhouette is differentiably rendered from the predicted mesh and compared with the labels. LiDAR consisteny is also applied    Self-supervised mesh reconstruction  Using the reconstructed 3D mesh, we can perform novel view warping of the source image to new target poses.\nWe build a large and diverse asset bank for simulation with over 8 000 unique vehicles.\nSimulation Pipeline GeoSim leverages the asset library to insert actors and simulate new images.\n  Step 1: Scenario Generation We first automatically generate a plausible placement and trajectory that complies with the existing traffic. An asset with a similar viewpoint is selected for rendering.   Automatic scenario generation \n  Step 2: Occlussion-aware rendering We render the selected asset at a new target pose. We account for occlusion by using dense depth from a pre-trained depth completion network. Additionally, we render a soft-shadow corresponding to the selected 3D asset considering a cloudy HDRI as the environement map..   Occlussion-aware rendering \n  Step 3: Post composition: Finally, we perform post-image composition. We use a synthesis network to handle inconsistent illumination and inpaint the discrepancies at the boundary, so that the vehicle is composited into the image seamlessly.   Post-image composition \n  Qualitative Results Video results in 4K resolution (4096x2160)\n\nCitation @inproceedings{chen2021geosim, title={GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving}, author={Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, Raquel Urtasun}, year={2021}, booktitle={CVPR}, }  ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"5f7982cda567ba3a966c271915f69c90","permalink":"https://tmux.top/publication/geosim/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/geosim/","section":"publication","summary":"\\[CVPR 2021 Oral\\] A geometry-aware image composition process (GeoSim) that synthesizes novel urban driving scenes by augmenting existing images with dynamic objects extracted from other scenes and rendered at novel poses.","tags":null,"title":"GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving","type":"publication"},{"authors":["Ming Liang","Bin Yang","Rui Hu","Yun Chen","Renjie Liao","Song Feng","Raquel Urtasun"],"categories":null,"content":"","date":1599004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599004800,"objectID":"22141d42bda5b3d90d8dde616c67df7b","permalink":"https://tmux.top/publication/lanegcn/","publishdate":"2020-09-02T00:00:00Z","relpermalink":"/publication/lanegcn/","section":"publication","summary":"\\[ECCV 2020 Oral\\] We propose a motion forecasting model that exploits a novel structured map representation as well as actor-map interactions. Instead of encoding vectorized maps as raster images, we construct a lane graph from raw map data to explicitly preserve the map structure. To capture the complex topology and long range dependencies of the lane graph, we propose LaneGCN which extends graph convolutions with multiple adjacency matrices and along-lane dilation. To capture the complex interactions between actors and maps, we exploit a fusion network consisting of four types of interactions, actor-to-lane, lane-to-lane, laneto-actor and actor-to-actor. Powered by LaneGCN and actor-map interactions, our model is able to predict accurate and realistic multi-modal trajectories. Our approach significantly outperforms the state-of-the-art on the large scale Argoverse motion forecasting benchmark.","tags":null,"title":"Learning Lane Graph Representations for Motion Forecasting","type":"publication"},{"authors":["Wenyuan Zeng","Shenlong Wang","Renjie Liao","Yun Chen","Bin Yang","Raquel Urtasun"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"1644d29a4b992acc136bfff6b8d9a4ac","permalink":"https://tmux.top/publication/dsdnet/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/dsdnet/","section":"publication","summary":"\\[ECCV 2020\\] In this paper, we propose the Deep Structured self-Driving Network (DSDNet), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, DSDNet explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.","tags":null,"title":"Dsdnet: Deep structured self-driving network","type":"publication"},{"authors":["Ming Liang","Bin Yang","Wenyuan Zeng","Yun Chen","Rui Hu","Sergio Casas","Raquel Urtasun"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"50554e279d997e7ce750094800330863","permalink":"https://tmux.top/publication/pnpnet/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/pnpnet/","section":"publication","summary":"\\[CVPR 2020\\]We tackle the problem of joint perception and motion forecasting in the context of self-driving vehicles. Towards this goal we propose PnPNet, an end-to-end model that takes as input sequential sensor data, and outputs at each time step object tracks and their future trajectories. The key component is a novel tracking module that generates object tracks online from detections and exploits trajectory level features for motion forecasting. Specifically, the object tracks get updated at each time step by solving both the data association problem and the trajectory estimation problem. Importantly, the whole model is end-to-end trainable and benefits from joint optimization of all tasks. We validate PnPNet on two large-scale driving datasets, and show significant improvements over the state-of-the-art with better occlusion recovery and more accurate future prediction.","tags":null,"title":"PnPNet: End-to-End Perception and Prediction with Tracking in the Loop","type":"publication"},{"authors":["Yun Chen","Bin Yang","Ming Liang","Raquel Urtasun"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"f627d8488c893f47b3df4f3855fa0bec","permalink":"https://tmux.top/publication/fusenet/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/fusenet/","section":"publication","summary":"\\[ICCV 2019\\] Depth completion using 2D-3D fusion. New SOTA achieved in KITTI.","tags":null,"title":"Learning Joint 2D-3D Representations for Depth Completion","type":"publication"},{"authors":["Ming Liang","Bin Yang","Yun Chen","Rui Hu","Raquel Urtasun"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"db14e0a1b6ad782683eab60b8e139006","permalink":"https://tmux.top/publication/mmf/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/mmf/","section":"publication","summary":"\\[CVPR 2019\\] In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this  goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and birdâ€™s eye view object detection, while being real-time","tags":null,"title":"Multi-task multi-sensor fusion for 3d object detection","type":"publication"},{"authors":["Yun Chen","Junxuan Chen","Bo Xiao","Zhengfang Wu","Ying Chi","Xuansong Xie","Xiansheng Hua"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"3eea6329f6d2bd056f06dceffb1d4eb1","permalink":"https://tmux.top/publication/vrcnn/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/vrcnn/","section":"publication","summary":"\\[ISBI 2019\\] Accurate and Efficient 3D Nodule detection with 3D R-CNN.","tags":["Source Themes"],"title":"Volume R-CNN: Unified Framework for CT Object Detection and Instance Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"80ad749dd8073174f20a3cb1be9d3c0b","permalink":"https://tmux.top/project/rcnn/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/rcnn/","section":"project","summary":"The **FIRST** pure PyTorch Faster R-CNN implementation, aiming at Simplicity and Readbility. Faster and Better.","tags":null,"title":"Faster R-CNN","type":"project"},{"authors":null,"categories":null,"content":"","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"c0a73ecfceeed4d63e71715ccfd7471e","permalink":"https://tmux.top/project/pytorch-book/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/pytorch-book/","section":"project","summary":"PyTorch book with anime generation, neural style, image caption and so on.","tags":null,"title":"Technical Book","type":"project"},{"authors":null,"categories":null,"content":"Here is a summary of my projects on 3D vision.\n  Camera Simulation for self-driving: Combine 3D reconstruction, image-based rendering and depth completion to create photo-realistic and geometrically consistent videos (Under Review)\n[PDF] [Video (4K Resolution)]\n    Depth Completion by joint-learning of appearance feature and geometry feature. (ICCV 2019)\n[PDF] [Qualitative Video] [Poster\n   Texture Reconstruction in the wild [WIP]\nSelf-supervised texture reconstruction with symmetry, inpainting and differential-rendering.\n  Please contact me if you want to talk in more detail.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"236a40aaf8e28b258151162d1406821e","permalink":"https://tmux.top/post/summary/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/summary/","section":"post","summary":"Here is a summary of my projects on 3D vision.\n  Camera Simulation for self-driving: Combine 3D reconstruction, image-based rendering and depth completion to create photo-realistic and geometrically consistent videos (Under Review)","tags":null,"title":"Projects on 3D Vision.","type":"post"}]