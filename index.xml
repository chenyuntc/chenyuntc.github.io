<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yun Chen</title>
    <link>https://tmux.top/</link>
      <atom:link href="https://tmux.top/index.xml" rel="self" type="application/rss+xml" />
    <description>Yun Chen</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 01 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://tmux.top/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Yun Chen</title>
      <link>https://tmux.top/</link>
    </image>
    
    <item>
      <title>GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving</title>
      <link>https://tmux.top/publication/geosim/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/geosim/</guid>
      <description>&lt;h3 id=&#34;overview-video&#34;&gt;Overview Video&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/_VLXc_VN0fE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube&lt;/a&gt; or &lt;a href=&#34;https://www.bilibili.com/video/BV1154y137jC/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bilibili&lt;/a&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/_VLXc_VN0fE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present GeoSim, a geometry-guided simulation procedure to insert dynamic objects into videos with greater realism. It helps enable scalable sensor simulation for training and testing autonomy systems, as well as applications like AR, VR, and video editing.
We first create 3D assets from real-world data and then use them for simulation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.gif&#34; alt=&#34;alternative text for search engines&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;3d-asset-reconstruction&#34;&gt;3D asset reconstruction&lt;/h3&gt;
&lt;p&gt;We propose a self-supervision model to automatically build vehicle asset bank from the wild without 3D groundtruth.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Multi-sensor feature is extracted from camera and LiDAR.&lt;/li&gt;
&lt;li&gt;Per-vertex deformation is applied on a learnable mean-shape to predict shape.&lt;/li&gt;
&lt;li&gt;Silhouette is differentiably rendered from the predicted mesh and compared with the labels. LiDAR consisteny is also applied&lt;/li&gt;
&lt;/ol&gt;






  



  
  











&lt;figure id=&#34;figure-self-supervised-mesh-reconstruction&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tmux.top/publication/geosim/imgs/meshnet_hua733d4b986cd68c9f0c34a5745b2df96_315378_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Self-supervised mesh reconstruction&#34;&gt;


  &lt;img data-src=&#34;https://tmux.top/publication/geosim/imgs/meshnet_hua733d4b986cd68c9f0c34a5745b2df96_315378_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1625&#34; height=&#34;554&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Self-supervised mesh reconstruction
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Using the reconstructed 3D mesh, we can perform novel view warping of the source image to new target poses.&lt;/p&gt;
&lt;p&gt;We build a large and diverse asset bank for simulation with over 8 000 unique vehicles.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;imgs/1.gif&#34; alt=&#34;Asset 1&#34;&gt;
&lt;img src=&#34;imgs/2.gif&#34; alt=&#34;Asset 2&#34;&gt;
&lt;img src=&#34;imgs/0.gif&#34; alt=&#34;Asset 3&#34;&gt;&lt;/p&gt;
&lt;!-- ![Asset 4](imgs/4.gif) --&gt;
&lt;h3 id=&#34;simulation-pipeline&#34;&gt;Simulation Pipeline&lt;/h3&gt;
&lt;p&gt;GeoSim leverages the asset library to insert actors and simulate new images.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Step 1: &lt;strong&gt;Scenario Generation&lt;/strong&gt;
We first automatically generate a plausible placement and trajectory that complies with the existing traffic.  An asset with a similar viewpoint is selected for rendering.






  



  
  











&lt;figure id=&#34;figure-automatic-scenario-generation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tmux.top/publication/geosim/imgs/scengen_hu44638c53dbe5dca9fe7c87469bc378b6_189941_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Automatic scenario generation&#34;&gt;


  &lt;img data-src=&#34;https://tmux.top/publication/geosim/imgs/scengen_hu44638c53dbe5dca9fe7c87469bc378b6_189941_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1096&#34; height=&#34;435&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Automatic scenario generation
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 2: &lt;strong&gt;Occlussion-aware rendering&lt;/strong&gt;
We then render the selected asset at a new target pose.
we account for occlusion by using dense depth from a pre-trained depth completion network.
Additionally, since our assets are 3d, we can render them inside a graphics engines to obtain soft shadows.






  



  
  











&lt;figure id=&#34;figure-test&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tmux.top/publication/geosim/imgs/occ-render_hu1402a7164d44a2700c8e9a8bdd06e53a_175614_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;test&#34;&gt;


  &lt;img data-src=&#34;https://tmux.top/publication/geosim/imgs/occ-render_hu1402a7164d44a2700c8e9a8bdd06e53a_175614_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1266&#34; height=&#34;360&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    test
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 3: &lt;strong&gt;Post composition:&lt;/strong&gt; Finally, we perform post-image composition. We use a synthesis network to handle inconsistent illumination and inpaint the discrepancies at the boundary,  so that the vehicle fits in seamlessly.






  



  
  











&lt;figure id=&#34;figure-post-image-composition&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tmux.top/publication/geosim/imgs/postcom_hu7e0ca6932088dd1dc406509373265a13_150365_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Post-image composition&#34;&gt;


  &lt;img data-src=&#34;https://tmux.top/publication/geosim/imgs/postcom_hu7e0ca6932088dd1dc406509373265a13_150365_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;852&#34; height=&#34;263&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Post-image composition
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;qualitative-results&#34;&gt;Qualitative Results&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://yun.sfo2.digitaloceanspaces.com/public/geosim/geosim-4K.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video results in 4K resolution (4096x2160)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://yun.sfo2.digitaloceanspaces.com/public/geosim/geosim-4K.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;imgs/4KVideo.png&#34; alt=&#34;4K video&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;imgs/c4-small.gif&#34; alt=&#34;ExampleSmall&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{chen2021geosim,
 title={GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving},
 author={Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, Raquel Urtasun},
 year={2021},
 booktitle={CVPR},
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Learning Lane Graph Representations for Motion Forecasting</title>
      <link>https://tmux.top/publication/lanegcn/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/lanegcn/</guid>
      <description>&lt;!-- 


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Dsdnet: Deep structured self-driving network</title>
      <link>https://tmux.top/publication/dsdnet/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/dsdnet/</guid>
      <description>&lt;!-- 


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>PnPNet: End-to-End Perception and Prediction with Tracking in the Loop</title>
      <link>https://tmux.top/publication/pnpnet/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/pnpnet/</guid>
      <description>&lt;!-- 


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Learning Joint 2D-3D Representations for Depth Completion</title>
      <link>https://tmux.top/publication/fusenet/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/fusenet/</guid>
      <description>&lt;!-- 


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Multi-task multi-sensor fusion for 3d object detection</title>
      <link>https://tmux.top/publication/mmf/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/mmf/</guid>
      <description>&lt;!-- 


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Volume R-CNN: Unified Framework for CT Object Detection and Instance Segmentation</title>
      <link>https://tmux.top/publication/vrcnn/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/publication/vrcnn/</guid>
      <description>&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;h1 id=&#34;click-the-cite-button-above-to-demo-the-feature-to-enable-visitors-to-import-publication-metadata-into-their-reference-management-software&#34;&gt;Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.&lt;/h1&gt;
&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;heading-1&#34;&gt;&lt;/h1&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;h1 id=&#34;click-the-slides-button-above-to-demo-academics-markdown-slides-feature&#34;&gt;Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.&lt;/h1&gt;
&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;supplementary-notes-can-be-added-here-including-code-and-mathhttpssourcethemescomacademicdocswriting-markdown-latex&#34;&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Faster R-CNN</title>
      <link>https://tmux.top/project/rcnn/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/project/rcnn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Technical Book</title>
      <link>https://tmux.top/project/pytorch-book/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/project/pytorch-book/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Projects on 3D Vision.</title>
      <link>https://tmux.top/post/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://tmux.top/post/summary/</guid>
      <description>&lt;p&gt;Here is a summary of my projects on 3D vision.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Camera Simulation&lt;/strong&gt; for self-driving: Combine 3D reconstruction, image-based rendering and depth completion to create photo-realistic and geometrically consistent videos  (Under Review)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://yun.sfo2.digitaloceanspaces.com/public/geosim-arxiv.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[PDF]&lt;/a&gt; &lt;a href=&#34;https://x.i3c.xyz/public/geosim.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Video (4K Resolution)]&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://x.i3c.xyz/tmp/a.gif&#34; alt=&#34;Geosim Teaser&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Depth Completion&lt;/strong&gt; by joint-learning of appearance feature and geometry feature. (ICCV 2019)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Learning_Joint_2D-3D_Representations_for_Depth_Completion_ICCV_2019_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[PDF]&lt;/a&gt;  &lt;a href=&#34;http://x.i3c.xyz/public/depth_qual.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Qualitative Video]&lt;/a&gt; [&lt;a href=&#34;https://tmux.top/publication/fusenet/iccv2019-fusenet-poster.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Poster&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://yun.sfo2.digitaloceanspaces.com/cloud/homepage/image-20201219153358788.png&#34; alt=&#34;Featured&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Texture Reconstruction&lt;/strong&gt; in the wild [WIP]&lt;/p&gt;
&lt;p&gt;Self-supervised texture reconstruction with symmetry, inpainting and differential-rendering.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://yun.sfo2.digitaloceanspaces.com/cloud/homepage/tex_rec.gif&#34; alt=&#34;tex_rec&#34;&gt;&lt;/p&gt;
&lt;p&gt;Please contact me if you want to talk in more detail.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
